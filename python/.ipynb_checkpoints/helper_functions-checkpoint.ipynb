{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Importing Jupyter notebook from models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math\n",
    "import gc\n",
    "import nbimporter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.fft import fft\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import optimizers, regularizers\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "print(\"Available GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "import models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "This notebook contains a number of helper functions useful for a variety of ML experiments (train-test, kFold cross validation, leave one subject out...). They are used by the corresponding experiment notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clear_old_dirs():** The function below clears old logs and checkpoints used in previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_old_dirs(which_data, which_experiment):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if which_experiment == \"train_test\":\n",
    "        if which_data == \"contact\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_train_test/\"):\n",
    "                os.remove(\"model_checkpoints/\"+which_data+\"_train_test/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_train_test/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_train_test/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_train_test/\"):\n",
    "                os.remove(\"results/\"+which_data+\"_train_test/\"+d)\n",
    "        elif which_data == \"radar\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_train_test/\"):\n",
    "                os.remove(\"model_checkpoints/\"+which_data+\"_train_test/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_train_test/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_train_test/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_train_test/\"):\n",
    "                os.remove(\"results/\"+which_data+\"_train_test/\"+d)\n",
    "        else:\n",
    "            print(\"Unknown data!\")\n",
    "            return\n",
    "    elif which_experiment == \"kfold\":\n",
    "        if which_data == \"contact\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_kfold/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_kfold/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_kfold/\"+d)\n",
    "        elif which_data == \"radar\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_kfold/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_kfold/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_kfold/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_kfold/\"+d)\n",
    "        else:\n",
    "            print(\"Unknown data!\")\n",
    "            return\n",
    "    elif which_experiment == \"loo\":\n",
    "        if which_data == \"contact\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_loo/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_loo/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_loo/\"+d)\n",
    "        elif which_data == \"radar\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_loo/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_loo/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_loo/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_loo/\"+d)\n",
    "        else:\n",
    "            print(\"Unknown data!\")\n",
    "            return\n",
    "    elif which_experiment == \"spectrograms\":\n",
    "        if which_data == \"contact\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_spectrograms/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_spectrograms/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_spectrograms/\"+d)\n",
    "        elif which_data == \"radar\":\n",
    "            for d in os.listdir(\"model_checkpoints/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"model_checkpoints/\"+which_data+\"_spectrograms/\"+d)\n",
    "            for d in os.listdir(\"model_training_logs/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"model_training_logs/\"+which_data+\"_spectrograms/\"+d)\n",
    "            for d in os.listdir(\"results/\"+which_data+\"_spectrograms/\"):\n",
    "                shutil.rmtree(\"results/\"+which_data+\"_spectrograms/\"+d)\n",
    "        else:\n",
    "            print(\"Unknown data!\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Unknown experiment type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scheduler():** A function to decrease the learning rate in accordance with some mathematical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr=0.1, decay=0.001):\n",
    "    if epoch <= 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 1/(1+decay*epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show_dist():** A function that shows the distribution of (class) values of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist(input_array):\n",
    "    display(pd.DataFrame(input_array, columns=[\"class\"]).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**oversample_smote():** The function below oversamples the data using Synthetic Minority Oversampling Technique, or SMOTE for short. It achieves equal class distribution in the dataset. It returns the oversampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_smote(X_all, Y_all):\n",
    "    oversample_X, oversample_Y = [], []\n",
    "    for i in range(X_all.shape[2]):\n",
    "        print(\"Oversampling signal\", i)\n",
    "        oversample = SMOTE(n_jobs=8)\n",
    "        X_t, Y_t = oversample.fit_resample(X_all[:,:,i], Y_all)\n",
    "        oversample_X.append(X_t)\n",
    "        oversample_Y.append(Y_t)\n",
    "\n",
    "    X_all = np.stack(oversample_X, axis=2)\n",
    "    Y_all = Y_t\n",
    "    print(\"Shapes after oversampling:\", X_all.shape, Y_all.shape)\n",
    "    \n",
    "    return X_all, Y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**shuffle_within_train_test():** The function below shuffles the data within the train and test split separately, as to not cause any neighbouring instances to appear in train and test. It returns the shuffled train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_within_train_test(X_train, Y_train, X_test, Y_test):\n",
    "    print(\"Shuffling WITHIN train/test, NOT overall!\")\n",
    "\n",
    "    indices_train = np.arange(Y_train.shape[0])\n",
    "    np.random.shuffle(indices_train)\n",
    "    X_train = X_train[indices_train]\n",
    "    Y_train = Y_train[indices_train]\n",
    "\n",
    "    indices_test = np.arange(Y_test.shape[0])\n",
    "    np.random.shuffle(indices_test)\n",
    "    X_test = X_test[indices_test]\n",
    "    Y_test = Y_test[indices_test]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**standardize_input():** Simple standard scaler (normalization) from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_input(X_train, X_test):\n",
    "    print(\"Scaling!\")\n",
    "    scaler = StandardScaler()\n",
    "    for i in range(X_train.shape[2]):\n",
    "        scaler = scaler.fit(X_train[:,:,i])\n",
    "        X_train[:,:,i] = scaler.transform(X_train[:,:,i])\n",
    "        X_test[:,:,i]  = scaler.transform(X_test[:,:,i])\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_model():** The function below creates a selected deep learning ANN architecture model and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(which_data, which_model, input_X, input_Y, hyperparams, metrics):\n",
    "    if which_data == \"contact\":\n",
    "        if which_model == \"fully_connected_small\":\n",
    "            model = models.create_contact_fully_connected_small(\n",
    "                input_data = input_X, \n",
    "                lr         = hyperparams[\"LR\"], \n",
    "                dropout    = hyperparams[\"DROPOUT\"],\n",
    "                n_classes  = to_categorical(input_Y).shape[1],\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        elif which_model == \"1d_cnn\":\n",
    "            model = models.cnn1d(\n",
    "                input_data = input_X, \n",
    "                hyperparams= hyperparams,\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        elif which_model == \"hybrid\":\n",
    "            model = models.hybrid_model(\n",
    "                input_data = input_X, \n",
    "                hyperparams= hyperparams,\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        else:\n",
    "            print(\"Uknown model type!\")\n",
    "            return None\n",
    "    elif which_data == \"radar\":\n",
    "        if which_model == \"fully_connected_small\":\n",
    "            model = models.create_radar_fully_connected_small(\n",
    "                input_data = input_X, \n",
    "                lr         = hyperparams[\"LR\"], \n",
    "                dropout    = hyperparams[\"DROPOUT\"],\n",
    "                n_classes  = to_categorical(input_Y).shape[1],\n",
    "                #n_classes  = 6,\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        elif which_model == \"1d_cnn\":\n",
    "            model = models.cnn1d(\n",
    "                input_data = input_X, \n",
    "                hyperparams= hyperparams,\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        elif which_model == \"hybrid\":\n",
    "            model = models.hybrid_model(\n",
    "                input_data = input_X, \n",
    "                hyperparams= hyperparams,\n",
    "                metrics    = metrics\n",
    "            )\n",
    "        else:\n",
    "            print(\"Uknown model type!\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Unknown data!\")\n",
    "        return None\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluate_model():** The function below evaluates a given model on a given (separate) test dataset. It returns a table with results and saves a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trained_model, X_test, Y_test, hyperparams, which_data, class_map, iteration, which_experiment, run_id):\n",
    "    # Table of numeric results\n",
    "    all_results = pd.DataFrame(columns=[\"loss\", \"tp\", \"fp\", \"tn\", \"fn\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n",
    "    \n",
    "    results = trained_model.evaluate(\n",
    "        x=[np.squeeze(x) for x in np.split(X_test, X_test.shape[2], axis=2)], # split individual signals\n",
    "        y=to_categorical(Y_test, num_classes=6), # one hot encode discrete labels\n",
    "        batch_size=hyperparams[\"BATCH_SIZE\"], \n",
    "        verbose=hyperparams[\"VERBOSE\"]\n",
    "    )\n",
    "    all_results.loc[0] = results[1:]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    predictions = trained_model.predict([np.squeeze(x) for x in np.split(X_test, X_test.shape[2], axis=2)])\n",
    "    Y_predicted = np.argmax(predictions, axis=1).astype('float')\n",
    "    \n",
    "    inv_class_map = {v: k for k, v in class_map.items()}\n",
    "    if np.unique(Y_test).size >= np.unique(Y_predicted).size:\n",
    "        larger = Y_test\n",
    "    else:\n",
    "        larger = Y_predicted\n",
    "    plot_classes = []\n",
    "    for val in np.unique(larger):\n",
    "        plot_classes.append(inv_class_map[int(val)])\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true=Y_test, y_pred=Y_predicted)\n",
    "    df_cm = pd.DataFrame(cm, index=plot_classes, columns=plot_classes)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.xlabel(\"PREDICTED\")\n",
    "    plt.ylabel(\"TRUE\")\n",
    "    \n",
    "    if which_experiment == \"kfold\" or which_experiment == \"loo\":\n",
    "        plt.savefig(\"results/\"+which_data+\"_\"+which_experiment+\"/\"+run_id+\"/\"+str(iteration)+\"/confusion_matrix.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.savefig(\"results/\"+which_data+\"_\"+which_experiment+\"/confusion_matrix.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**execute_single_fold():** The function below executes a single fold of the cross validation procedure, obtaining a set of results for the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_single_fold(which_data, which_model, train_index, test_index, X_all, Y_all, hyperparams, metrics, iteration, class_map):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        \n",
    "    # Take n-1 folds for train and 1 for test\n",
    "    X_train = X_all[train_index, :, :]\n",
    "    X_test  = X_all[test_index, :, :]\n",
    "    Y_train = Y_all[train_index]\n",
    "    Y_test  = Y_all[test_index]\n",
    "    print(\"Train shapes:\", X_train.shape, Y_train.shape, \"Test shapes:\", X_test.shape, Y_test.shape)\n",
    "    show_dist(Y_train)\n",
    "    show_dist(Y_test)\n",
    "    \n",
    "    # Random shuffling within train test (no overall)\n",
    "    if hyperparams[\"SHUFFLE\"]:\n",
    "        X_train, Y_train, X_test, Y_test = shuffle_within_train_test(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "    # Standardize/scale the data\n",
    "    if hyperparams[\"STANDARD\"]:\n",
    "        X_train, X_test = standardize_input(X_train, X_test)\n",
    "\n",
    "    # Define fully-connected (contact or radar) signal model\n",
    "    model = get_model(which_data, which_model, X_train, Y_train, hyperparams, metrics)\n",
    "\n",
    "    if not os.path.exists(\"model_checkpoints/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)) or not os.path.exists(\"model_training_logs/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)) or not os.path.exists(\"results/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)):\n",
    "        os.makedirs(\"model_checkpoints/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration))\n",
    "        os.makedirs(\"model_training_logs/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration))\n",
    "        os.makedirs(\"results/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration))\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=[np.squeeze(x) for x in np.split(X_train, X_train.shape[2], axis=2)], # split individual signals\n",
    "        y=to_categorical(Y_train, num_classes=6), # one hot encode discrete labels\n",
    "        batch_size=hyperparams[\"BATCH_SIZE\"],\n",
    "        epochs=hyperparams[\"N_EPOCHS\"],\n",
    "        verbose=hyperparams[\"VERBOSE\"],\n",
    "        validation_split=hyperparams[\"VALIDATION\"],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoints/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)+\"/best_model.hdf5\", monitor='val_loss', mode='min', save_best_only=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"model_training_logs/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)),\n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on left-out test data\n",
    "    trained_model = load_model(filepath=\"model_checkpoints/\"+which_data+\"_kfold/\"+hyperparams[\"RUN_ID\"]+\"/\"+str(iteration)+\"/best_model.hdf5\")\n",
    "    results = evaluate_model(trained_model, X_test, Y_test, hyperparams, which_data, class_map, iteration, which_experiment=\"kfold\", run_id=hyperparams[\"RUN_ID\"])\n",
    "    \n",
    "    # Cleanup for GPU memory\n",
    "    del model\n",
    "    del trained_model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**execute_single_loo_iteration():** The function below executes a single iteration of the leave-one-out procedure, obtaining a set of results for the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_single_loo_iteration(which_data, which_model, train_index, test_index, X_loo, Y_loo, hyperparams, metrics, iteration, class_map):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        \n",
    "    # Take n-1 for train and 1 for test\n",
    "    X_train = np.concatenate(np.asarray(X_loo, dtype=object)[train_index], axis=0)\n",
    "    X_test = np.concatenate(np.asarray(X_loo, dtype=object)[test_index], axis=0)\n",
    "    Y_train = np.concatenate(np.asarray(Y_loo, dtype=object)[train_index], axis=0)\n",
    "    Y_test = np.concatenate(np.asarray(Y_loo, dtype=object)[test_index], axis=0)\n",
    "    print(\"Train shapes:\", X_train.shape, Y_train.shape, \"Test shapes:\", X_test.shape, Y_test.shape)\n",
    "    \n",
    "    # Oversample within train and test (left-out subject) separately, since distribution is skewed in both\n",
    "    if hyperparams[\"OVERSAMPLE\"]:\n",
    "        if np.unique(Y_test).size <= 1:\n",
    "            print(\"Test data (left-out subject) has only 1 class:\", np.unique(Y_test), \" so it doesn't make sense to oversample!\")\n",
    "        else:\n",
    "            # Oversample minority classes in a smart way using SMOTE\n",
    "            print(\"Oversampling the train (n-1) subjects, with shapes:\", X_train.shape, Y_train.shape)\n",
    "            X_train, Y_train = oversample_smote(X_train, Y_train)\n",
    "            print(\"Oversampling the test (left-out) subject with shapes:\", X_test.shape, Y_test.shape)\n",
    "            if X_test.shape[0] >= 10:\n",
    "                X_test, Y_test = oversample_smote(X_test, Y_test)\n",
    "    \n",
    "    show_dist(Y_train)\n",
    "    show_dist(Y_test)\n",
    "    \n",
    "    # Random shuffling within train test (no overall)\n",
    "    if hyperparams[\"SHUFFLE\"]:\n",
    "        X_train, Y_train, X_test, Y_test = shuffle_within_train_test(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    # Standardize/scale the data\n",
    "    if hyperparams[\"STANDARD\"]:\n",
    "        X_train, X_test = standardize_input(X_train, X_test)\n",
    "\n",
    "    # Define fully-connected (contact or radar) signal model\n",
    "    model = get_model(which_data, which_model, X_train, Y_train, hyperparams, metrics)\n",
    "\n",
    "    if not os.path.exists(\"model_checkpoints/\"+which_data+\"_loo/\"+str(iteration)) or not os.path.exists(\"model_training_logs/\"+which_data+\"_loo/\"+str(iteration)) or not os.path.exists(\"results/\"+which_data+\"_loo/\"+str(iteration)):\n",
    "        os.makedirs(\"model_checkpoints/\"+which_data+\"_loo/\"+str(iteration))\n",
    "        os.makedirs(\"model_training_logs/\"+which_data+\"_loo/\"+str(iteration))\n",
    "        os.makedirs(\"results/\"+which_data+\"_loo/\"+str(iteration))\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=[np.squeeze(x) for x in np.split(X_train, X_train.shape[2], axis=2)], # split individual signals\n",
    "        y=to_categorical(Y_train, num_classes=np.unique(Y_train).size), # one hot encode discrete labels\n",
    "        batch_size=hyperparams[\"BATCH_SIZE\"],\n",
    "        epochs=hyperparams[\"N_EPOCHS\"],\n",
    "        verbose=hyperparams[\"VERBOSE\"],\n",
    "        validation_split=hyperparams[\"VALIDATION\"],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoints/\"+which_data+\"_loo/\"+str(iteration)+\"/best_model.hdf5\", monitor='val_loss', mode='min', save_best_only=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"model_training_logs/\"+which_data+\"_loo/\"+str(iteration)),\n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on left-out test data\n",
    "    trained_model = load_model(filepath=\"model_checkpoints/\"+which_data+\"_loo/\"+str(iteration)+\"/best_model.hdf5\")\n",
    "    results = evaluate_model(trained_model, X_test, Y_test, hyperparams, which_data, class_map, iteration, which_experiment=\"loo\")\n",
    "    \n",
    "    # Cleanup for GPU memory\n",
    "    del model\n",
    "    del trained_model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot_cv_indices():** The function below plots exact split of indices in the kFold CV, for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=plt.cm.coolwarm,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_splits + 1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main train-test experiment\n",
    "\n",
    "**train_test_experiment():** The function below calls all other functions to read the data, split it, define and train a model and evaluate it in train-test split experiment. Finally, it does some memory cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_experiment(which_data, which_model, hyperparams, metrics, class_map, selected_classes=None):\n",
    "    # Clean old things from previous run\n",
    "    clear_old_dirs(which_data, \"train_test\")\n",
    "    \n",
    "    # Read the full data\n",
    "    X_all = np.load(\"model_inputs_and_targets/train_test/X_all_\"+which_data+\".npy\")\n",
    "    Y_all = np.load(\"model_inputs_and_targets/train_test/Y_all.npy\")\n",
    "    print(\"Original shape:\", X_all.shape, Y_all.shape)\n",
    "    \n",
    "    if selected_classes:\n",
    "        # Make subsets of data of selected classes\n",
    "        print(\"Taking subsets of data for classes:\", selected_classes)\n",
    "        X_new, Y_new = [], []\n",
    "        for selected_class in selected_classes:\n",
    "            idx = (Y_all == class_map[selected_class]).nonzero()\n",
    "            X_new.append(X_all[idx[0], :, :])\n",
    "            Y_new.append(Y_all[idx])\n",
    "\n",
    "        X_all = np.concatenate(X_new, axis=0)\n",
    "        Y_all = np.concatenate(Y_new, axis=0)\n",
    "        print(\"Subset shapes:\", X_all.shape, Y_all.shape)\n",
    "        \n",
    "        class_map_new = {selected_class: class_map[selected_class] for selected_class in selected_classes}\n",
    "        class_map = class_map_new\n",
    "    \n",
    "    if hyperparams[\"OVERSAMPLE\"]:\n",
    "        # Oversample minority classes in a smart way using SMOTE\n",
    "        X_all, Y_all = oversample_smote(X_all, Y_all)\n",
    "        \n",
    "    # Withold test data\n",
    "    split_idx = math.ceil(hyperparams[\"TRAIN_AMNT\"]*Y_all.shape[0])\n",
    "    X_train   = X_all[:split_idx,:,:]\n",
    "    Y_train   = Y_all[:split_idx]\n",
    "    X_test    = X_all[split_idx:,:,:]\n",
    "    Y_test    = Y_all[split_idx:]\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=1-train_amnt, shuffle=False, stratify=Y_all)\n",
    "    \n",
    "    # Random shuffling within train test (no overall)\n",
    "    if hyperparams[\"SHUFFLE\"]:\n",
    "        X_train, Y_train, X_test, Y_test = shuffle_within_train_test(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    # Standardize/scale the data\n",
    "    if hyperparams[\"STANDARD\"]:\n",
    "        X_train, X_test = standardize_input(X_train, X_test)\n",
    "            \n",
    "    # Define fully-connected (contact or radar) signal model\n",
    "    model = get_model(which_data, which_model, X_train, Y_train, hyperparams, metrics)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x=[np.squeeze(x) for x in np.split(X_train, X_train.shape[2], axis=2)], # split individual signals\n",
    "        y=to_categorical(Y_train, num_classes=6), # one hot encode discrete labels\n",
    "        batch_size=hyperparams[\"BATCH_SIZE\"],\n",
    "        epochs=hyperparams[\"N_EPOCHS\"],\n",
    "        verbose=hyperparams[\"VERBOSE\"],\n",
    "        validation_split=hyperparams[\"VALIDATION\"],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoints/\"+which_data+\"_train_test/best_model.hdf5\", monitor='val_loss', mode='min', save_best_only=True),\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"model_training_logs/\"+which_data+\"_train_test/\"),\n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on left-out test data\n",
    "    trained_model = load_model(filepath=\"model_checkpoints/\"+which_data+\"_train_test/best_model.hdf5\")\n",
    "    all_results = evaluate_model(trained_model, X_test, Y_test, hyperparams, which_data, class_map, iteration=-1, which_experiment=\"train_test\")\n",
    "    \n",
    "    # Cleanup for GPU memory\n",
    "    del model\n",
    "    del trained_model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main k-fold CV experiment\n",
    "\n",
    "**kfold_cv_experiment():** The function below calls all other functions to read the data, split it, define and train a model and evaluate it in k-fold cross validation experiment. Finally, it does some memory cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv_experiment(which_data, which_model, hyperparams, metrics, class_map, n_folds=10, selected_classes=None):\n",
    "    # Clean old things from previous run\n",
    "    #clear_old_dirs(which_data, \"kfold\")\n",
    "    \n",
    "    # Read the full data\n",
    "    X_all = np.load(\"model_inputs_and_targets/kfold/X_all_\"+which_data+\".npy\")\n",
    "    Y_all = np.load(\"model_inputs_and_targets/kfold/Y_all.npy\")\n",
    "    print(\"Original shape:\", X_all.shape, Y_all.shape)\n",
    "    \n",
    "    if selected_classes:\n",
    "        # Make subsets of data of selected classes\n",
    "        print(\"Taking subsets of data for classes:\", selected_classes)\n",
    "        X_new, Y_new = [], []\n",
    "        for selected_class in selected_classes:\n",
    "            idx = (Y_all == class_map[selected_class]).nonzero()\n",
    "            X_new.append(X_all[idx[0], :, :])\n",
    "            Y_new.append(Y_all[idx])\n",
    "\n",
    "        X_all = np.concatenate(X_new, axis=0)\n",
    "        Y_all = np.concatenate(Y_new, axis=0)\n",
    "        print(\"Subset shapes:\", X_all.shape, Y_all.shape)\n",
    "        \n",
    "        class_map_new = {selected_class: class_map[selected_class] for selected_class in selected_classes}\n",
    "        class_map = class_map_new\n",
    "    \n",
    "    if hyperparams[\"OVERSAMPLE\"]:\n",
    "        # Oversample minority classes in a smart way using SMOTE\n",
    "        X_all, Y_all = oversample_smote(X_all, Y_all)\n",
    "        \n",
    "    if hyperparams[\"FFT_ONLY\"]:\n",
    "        print(\"Transforming temporal data to 1D FFT (abs squared)...\")\n",
    "        for signal in range(X_all.shape[2]):\n",
    "            i = 0\n",
    "            for row in X_all[:,:,signal]:\n",
    "                X_all[i, :, signal] = np.abs(fft(row))**2\n",
    "                i += 1\n",
    "    \n",
    "    if hyperparams[\"ADD_FFT\"]:\n",
    "        # FFT\n",
    "        print(\"Adding 1D FFT (abs squared) to temporal data...\")\n",
    "        X_all_fft = np.copy(X_all)\n",
    "        for signal in range(X_all.shape[2]):\n",
    "            i = 0\n",
    "            for row in X_all[:,:,signal]:\n",
    "                X_all_fft[i,:,signal] = np.abs(fft(row))**2\n",
    "                i += 1\n",
    "        \n",
    "        X_all = np.concatenate([X_all[:,:,:], X_all_fft[:,:,:]], axis=2)\n",
    "        print(X_all.shape)\n",
    "        \n",
    "    # Group_by class (for visualisation purposes)\n",
    "    idx = np.argsort(Y_all)\n",
    "    X_all = X_all[idx]\n",
    "    Y_all = Y_all[idx]\n",
    "        \n",
    "    # Prepare kFold CV\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    print(\"Number of splits:\", skf.get_n_splits(X_all, Y_all))\n",
    "    \n",
    "    # Sanity plot of index splits\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_cv_indices(skf, X_all, Y_all, ax, n_folds)\n",
    "    plt.show()\n",
    "\n",
    "    iteration = 0\n",
    "    all_results = pd.DataFrame(columns=[\"loss\", \"tp\", \"fp\", \"tn\", \"fn\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n",
    "    for train_index, test_index in skf.split(X_all, Y_all):\n",
    "        print(\"====================== FOLD:\", iteration, \"======================\")\n",
    "        \n",
    "        # Sanity check\n",
    "        print(\"Train:\", train_index)\n",
    "        print(\"Test:\", test_index)\n",
    "        print(\"Intersection:\", list(set(train_index) & set(test_index)))\n",
    "        \n",
    "        results = execute_single_fold(which_data, which_model, train_index, test_index, X_all, Y_all, hyperparams, metrics, iteration, class_map)\n",
    "        all_results = all_results.append(results)\n",
    "\n",
    "        iteration += 1\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main LOO experiment\n",
    "\n",
    "**loo_experiment():** The function below calls other functions to read the data, split it, define and train a model, and evaluate it in a LOO experiment. Finally, it does some memory cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_experiment(which_data, which_model, hyperparams, metrics, class_map, selected_classes=None, remove_incomplete_data=False):\n",
    "    # Clean old things from previous run\n",
    "    clear_old_dirs(which_data, \"loo\")\n",
    "    \n",
    "    # Read full data\n",
    "    X_loo = list(np.load(\"model_inputs_and_targets/leave_one_out/X_loo_contact.npy\", allow_pickle=True)[()].values())\n",
    "    Y_loo = list(np.load(\"model_inputs_and_targets/leave_one_out/Y_loo.npy\", allow_pickle=True)[()].values())\n",
    "    \n",
    "    # Remove subjects with incomplete data\n",
    "    if remove_incomplete_data:\n",
    "        X_loo_copy = X_loo\n",
    "        Y_loo_copy = Y_loo\n",
    "        for i, subject in enumerate(Y_loo_copy):\n",
    "            if len(Counter(subject).keys()) < 5:\n",
    "                X_loo.pop(i)\n",
    "                Y_loo.pop(i)\n",
    "    \n",
    "    print(\"Full shapes:\", np.concatenate(X_loo, axis=0).shape, np.concatenate(Y_loo, axis=0).shape)\n",
    "    \n",
    "    if selected_classes:\n",
    "        # Make subsets of data of selected classes\n",
    "        print(\"Taking subsets of data for classes:\", selected_classes)\n",
    "        X_new, Y_new = [], []\n",
    "        for selected_class in selected_classes:\n",
    "            idx = (Y_loo == class_map[selected_class]).nonzero()\n",
    "            X_new.append(X_loo[idx[0], :, :])\n",
    "            Y_new.append(Y_loo[idx])\n",
    "\n",
    "        X_loo = np.concatenate(X_new, axis=0)\n",
    "        Y_loo = np.concatenate(Y_new, axis=0)\n",
    "        print(\"Subset shapes:\", X_loo.shape, Y_loo.shape)\n",
    "        \n",
    "        class_map_new = {selected_class: class_map[selected_class] for selected_class in selected_classes}\n",
    "        class_map = class_map_new\n",
    "    \n",
    "    # Prepare leave one out\n",
    "    loo = LeaveOneOut()\n",
    "    print(\"Number of splits:\", loo.get_n_splits(X_loo), loo.get_n_splits(Y_loo), \"\\n\")\n",
    "    \n",
    "    iteration = 0\n",
    "    all_results = pd.DataFrame(columns=[\"loss\", \"tp\", \"fp\", \"tn\", \"fn\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n",
    "    for train_index, test_index in loo.split(X_loo):\n",
    "        print(\"====================== ITERATION:\", iteration, \"======================\") \n",
    "        results = execute_single_loo_iteration(which_data, which_model, train_index, test_index, X_loo, Y_loo, hyperparams, metrics, iteration, class_map)\n",
    "        all_results = all_results.append(results)\n",
    "\n",
    "        iteration += 1\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main spectrograms experiment\n",
    "\n",
    "**spectrograms_experiment():** The function below calls other functions to read the data, split it, define and train a model, and evaluate it in a k-fold experiment using spectrograms. Finally, it does some memory cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, which_data, batch_size, dim, n_channels=6, n_classes=6, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.which_data = which_data\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp, indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            \n",
    "            # Store sample\n",
    "            X[i,:,:,:] = np.load(\"model_inputs_and_targets/spectrograms/\" + self.which_data + \"/\" + ID)\n",
    "            \n",
    "            # Store class\n",
    "            y[i] = self.labels[indexes[i]]\n",
    "        \n",
    "        return ([x for x in np.split(X, X.shape[3], axis=3)], keras.utils.to_categorical(y, num_classes=self.n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrograms_experiment(which_data, which_model, hyperparams, metrics, class_map, n_folds, selected_classes=None):\n",
    "    # Clean old things from previous run\n",
    "    clear_old_dirs(which_data, \"spectrograms\")\n",
    "    \n",
    "    # Read full data\n",
    "    base_path = \"model_inputs_and_targets/spectrograms/\"\n",
    "    X_dict, Y_dict = {}, {}\n",
    "    for class_name in os.listdir(base_path+which_data):\n",
    "        X_dict[class_name] = np.array([class_name+\"/\"+file for file in os.listdir(base_path+which_data+\"/\"+class_name)])\n",
    "        Y_dict[class_name] = np.array([class_map[class_name] for file in os.listdir(base_path+which_data+\"/\"+class_name)])\n",
    "\n",
    "    # Prepare kFold CV\n",
    "    X_full = np.concatenate([x for x in X_dict.values()])\n",
    "    Y_full = np.concatenate([y for y in Y_dict.values()])\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "    print(\"Number of splits:\", skf.get_n_splits(X_full), skf.get_n_splits(Y_full), \"\\n\")\n",
    "    \n",
    "    iteration = 0\n",
    "    all_results = pd.DataFrame(columns=[\"loss\", \"tp\", \"fp\", \"tn\", \"fn\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n",
    "    for train_idx, test_idx in skf.split(X_full, Y_full):\n",
    "        print(\"========== Iteration:\", iteration, \"==========\")\n",
    "        X_train_files = X_full[train_idx[0:int(0.7*train_idx.shape[0])]]\n",
    "        X_val_files = X_full[train_idx[int(0.7*train_idx.shape[0]):]]\n",
    "        X_test_files = X_full[test_idx]\n",
    "        Y_train = Y_full[train_idx[0:int(0.7*train_idx.shape[0])]]\n",
    "        Y_val = Y_full[train_idx[int(0.7*train_idx.shape[0]):]]\n",
    "        Y_test = Y_full[test_idx]\n",
    "        \n",
    "        print(X_test_files)\n",
    "        print(Y_test)\n",
    "        \n",
    "        # Make a model\n",
    "        example_instance = np.load(base_path+which_data+\"/\"+X_train_files[0])\n",
    "        model = models.create_2d_CNN_small(example_instance, kernel_size=(3,3), n_classes=6)\n",
    "        \n",
    "        # Generator\n",
    "        train_gen = DataGenerator(X_train_files, Y_train, which_data, hyperparams[\"BATCH_SIZE\"], dim=example_instance.shape)\n",
    "        val_gen = DataGenerator(X_val_files, Y_val, which_data, hyperparams[\"BATCH_SIZE\"], dim=example_instance.shape)\n",
    "        test_gen = DataGenerator(X_test_files, Y_test, which_data, hyperparams[\"BATCH_SIZE\"], dim=example_instance.shape)\n",
    "        \n",
    "        model.fit(x=train_gen, \n",
    "                  validation_data=val_gen, \n",
    "                  use_multiprocessing=True, \n",
    "                  workers=8, \n",
    "                  epochs=hyperparams[\"N_EPOCHS\"],\n",
    "                  verbose=hyperparams[\"VERBOSE\"],\n",
    "                  callbacks=[\n",
    "                        tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoints/\"+which_data+\"_spectrograms/best_model.hdf5\", monitor='val_loss', mode='min', save_best_only=True),\n",
    "                        tf.keras.callbacks.TensorBoard(log_dir=\"model_training_logs/\"+which_data+\"_spectrograms/\"),\n",
    "                        tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "                  ])\n",
    "        \n",
    "        iteration += 1\n",
    "        break\n",
    "        \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
